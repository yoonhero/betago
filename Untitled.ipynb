{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdec4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdbc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"dataset/raw/gomocup2022results/Freestyle*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ff335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14104"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f84803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Name                 Elo    +    - games score oppo. draws \n",
      "1. RAPFI22 2189\n",
      "<HTML>\n",
      "[White \"0#EMBRYO22\"]\n",
      "[2022-04-17 18:56:22] Server started.\n",
      "                    3#R 2#K 1#B 0#E 5#A 4#Y 6#P 8#S 7#X 9#R 10# 12# 11# 13#\n",
      "(0, 1, 0)\t2\t0\t827222\t927444\t56\t56\n",
      "Rank Name              Elo    +    - games score oppo. draws \n",
      "1. WINE18 2461\n",
      "<HTML>\n",
      "[White \"0#WINE18\"]\n",
      "[2022-04-16 10:59:36] Server started.\n",
      "                 0#W 2#S 3#H 4#T 5#D 1#W 6#S 8#C 7#G 9#O 40# 10# 12# 13# 14# 11# 15# 17# 16# 39# 18# 19# 20# 22# 21# 23# 25# 24# 28# 26# 42# 27# 29# 41# 32# 31# 30# 33# 34# 35# 36# 37# 38#\n",
      "(0, 1, 0)\t2\t0\t86904\t43222\t24\t24\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    try:\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read().splitlines()[0]\n",
    "\n",
    "        winner = text.split(\",\")[-1].strip()\n",
    "        if winner != \"0\":\n",
    "            print(text)\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42926b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,2,0',\n",
       " '2,4,0',\n",
       " '4,2,0',\n",
       " '4,4,0',\n",
       " '6,2,0',\n",
       " '2,6,0',\n",
       " '5,7,0',\n",
       " '5,4,73375',\n",
       " '3,4,45430',\n",
       " '6,5,66264',\n",
       " '5,2,33615',\n",
       " '3,2,4',\n",
       " '4,3,32561',\n",
       " '2,5,60730',\n",
       " '2,7,20584',\n",
       " '4,7,56723',\n",
       " '5,6,14399',\n",
       " '4,8,55927',\n",
       " '4,5,22068',\n",
       " '6,7,54480',\n",
       " '5,3,40566',\n",
       " '6,3,49555',\n",
       " '6,4,18886',\n",
       " '7,5,45574',\n",
       " '5,5,17646',\n",
       " '4,6,42623',\n",
       " '5,8,17443',\n",
       " '5,9,6',\n",
       " '6,10,18333',\n",
       " '9,5,42694',\n",
       " '10,5,16332',\n",
       " '9,8,37181',\n",
       " '8,7,26050',\n",
       " '9,6,34740',\n",
       " '9,7,26329',\n",
       " '10,7,34552',\n",
       " '8,8,29437',\n",
       " '7,9,32318',\n",
       " '8,9,23177',\n",
       " '8,6,29209',\n",
       " '6,8,15335',\n",
       " '10,6,31238',\n",
       " '8,2,21306',\n",
       " '7,2,22',\n",
       " '7,3,22115',\n",
       " '9,1,5',\n",
       " '11,6,21537',\n",
       " '9,4,25358',\n",
       " '6,6,28021',\n",
       " '11,3,24750',\n",
       " '7,7,13052',\n",
       " '9,9,6',\n",
       " '8,5,19443',\n",
       " '12,4,21948',\n",
       " '10,4,18348',\n",
       " '12,6,20240',\n",
       " '10,2,98',\n",
       " '12,8,5709',\n",
       " '8,10,270',\n",
       " '8,11,6',\n",
       " '11,7,145',\n",
       " '9,2,4',\n",
       " '9,3,6',\n",
       " '1,5,5',\n",
       " '3,7,6',\n",
       " '12,7,6',\n",
       " '12,5,4',\n",
       " '4,9,4',\n",
       " '4,10,5',\n",
       " '3,9,6',\n",
       " '6,9,6',\n",
       " '1,9,6',\n",
       " '2,9,6',\n",
       " '12,9,6',\n",
       " '12,10,6',\n",
       " '11,8,4',\n",
       " '13,10,5',\n",
       " '10,8,4',\n",
       " '13,8,4',\n",
       " '10,9,6',\n",
       " '9,10,6',\n",
       " '10,10,6',\n",
       " 'HEWER18.zip']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(files[0], \"r\") as f:\n",
    "    text = f.read().splitlines()\n",
    "text[1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9fdf882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gomuku import GoMuKuBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fca342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O |   | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X |   | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X |   | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X |   | O | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X |   |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X |   |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O |   |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O |   |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X |   |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O |   | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O |   | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O |   |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X |   |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \u001b[H\u001b[2J\n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O |   |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X |   | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O | X |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   |   |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O | X |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   |   | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O | X |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X |   | O | X | O | X | X | X | O | X |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X |   |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   |   | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   | O | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   |   |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   |   | O | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   | O | O | O |   | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   |   |   | O | O | O | X | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   | O |   | O | O | O | X | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   |   |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   |   |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   | X |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   | O | O |   |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   | X |   |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O |   | O | O |   |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   | X | X |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O | O | O | O |   |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   | X | X |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O | O | O | O | X |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O |   |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   | X | X |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O | O | O | O | X |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O | O |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X |   |   |   | X | X |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O | O | O | O | X |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O | O |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X | X |   |   | X | X |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
      "0 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "1 |   |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |  \n",
      "2 |   |   | X | O | X | X | X | O | X | O | X |   |   |   |   |   |   |   |   |  \n",
      "3 |   |   |   |   | X | X | O | X |   | X |   | O |   |   |   |   |   |   |   |  \n",
      "4 |   |   | O | X | O | O | X |   |   | O | X |   | O |   |   |   |   |   |   |  \n",
      "5 |   | O | O |   | X | X | O | O | X | O | X |   | X |   |   |   |   |   |   |  \n",
      "6 |   |   | O |   | O | X | X |   | O | O | O | X | O |   |   |   |   |   |   |  \n",
      "7 |   |   | X | X | O | X | O | X | X | X | O | X | O |   |   |   |   |   |   |  \n",
      "8 |   |   |   |   | O | X | X |   | X | O | O | O | O | X |   |   |   |   |   |  \n",
      "9 |   | O | X | O | O | O | X | O | X | O | O |   | O |   |   |   |   |   |   |  \n",
      "10 |   |   |   |   | X |   | X |   | X | X | O |   | X | X |   |   |   |   |   |  \n",
      "11 |   |   |   |   |   |   |   |   | O |   |   |   |   |   |   |   |   |   |   |  \n",
      "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
      "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "board = GoMuKuBoard(nrow=20, ncol=20, n_to_win=5)\n",
    "for pos in text[1:-4]:\n",
    "    x, y = pos.split(\",\")[:-1]\n",
    "    x, y = int(x), int(y)\n",
    "    board.set(x, y)\n",
    "    os.system('clear')\n",
    "    print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6fbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4444f5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\yoonhero\\alphagomu\\Untitled.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/yoonhero/alphagomu/Untitled.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m0.1\u001b[39m], [\u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/yoonhero/alphagomu/Untitled.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m0\u001b[39m], [\u001b[39m1\u001b[39m]])\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/yoonhero/alphagomu/Untitled.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0.1], [1]]).to(torch.float32)\n",
    "gt = torch.tensor([[0], [1]]).to(torch.float32)\n",
    "criterion = torch.nn.BCELoss()\n",
    "criterion(a, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87a87c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (convs): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (res_block): Sequential(\n",
      "        (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (res_block): Sequential(\n",
      "        (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "      (down): Sequential(\n",
      "        (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (res_block): Sequential(\n",
      "        (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (res_block): Sequential(\n",
      "        (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "      (down): Sequential(\n",
      "        (0): Conv2d(8, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ff): Sequential(\n",
      "    (0): Rearrange('b c w h -> b (c w h)')\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=500, out_features=1, bias=True)\n",
      "    (6): Sigmoid()\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): ResBlock(\n",
      "    (res_block): Sequential(\n",
      "      (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (1): ResBlock(\n",
      "    (res_block): Sequential(\n",
      "      (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (activation): ReLU()\n",
      "    (down): Sequential(\n",
      "      (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (2): ResBlock(\n",
      "    (res_block): Sequential(\n",
      "      (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (3): ResBlock(\n",
      "    (res_block): Sequential(\n",
      "      (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (activation): ReLU()\n",
      "    (down): Sequential(\n",
      "      (0): Conv2d(8, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ResBlock(\n",
      "  (res_block): Sequential(\n",
      "    (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ResBlock(\n",
      "  (res_block): Sequential(\n",
      "    (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      "  (down): Sequential(\n",
      "    (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Conv2d(2, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Conv2d(2, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ResBlock(\n",
      "  (res_block): Sequential(\n",
      "    (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "ResBlock(\n",
      "  (res_block): Sequential(\n",
      "    (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      "  (down): Sequential(\n",
      "    (0): Conv2d(8, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(8, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Conv2d(8, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (0): Conv2d(8, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Conv2d(8, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Sequential(\n",
      "  (0): Rearrange('b c w h -> b (c w h)')\n",
      "  (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=500, out_features=1, bias=True)\n",
      "  (6): Sigmoid()\n",
      ")\n",
      "Rearrange('b c w h -> b (c w h)')\n",
      "Linear(in_features=500, out_features=500, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=500, out_features=500, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=500, out_features=1, bias=True)\n",
      "Sigmoid()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from base import SimpleCNN\n",
    "to_feed = torch.zeros((1, 2, 20, 20))\n",
    "net = SimpleCNN(20, 20, [2, 8, 20])\n",
    "net(to_feed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65b3a380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "img = torch.zeros((3, 4, 4 ))\n",
    "img[:, 0, 0] = torch.tensor([255, 255, 255])/256\n",
    "# img = rearrange(img, \"h w c -> c h w\")\n",
    "a = transforms.ToPILImage()\n",
    "im = a(img).resize((256, 256), resample=Image.BOX)\n",
    "\n",
    "# img = torch.randn((3, 5, 5))\n",
    "to_pil_image(img).resize((256, 256))/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede2ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gomuku import GoMuKuBoard\n",
    "\n",
    "board = GoMuKuBoard(20, 20, 5)\n",
    "board.set(0, 0)\n",
    "board.set(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ea13d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19\n",
       "0 | X |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "1 | O |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "2 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "3 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "4 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "5 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "6 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "7 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "8 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "9 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "10 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "11 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "13 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "14 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "15 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "16 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "17 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "18 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  \n",
       "19 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6edfebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 2],\n",
       "  [0, 3],\n",
       "  [0, 4],\n",
       "  [0, 5],\n",
       "  [0, 6],\n",
       "  [0, 7],\n",
       "  [0, 8],\n",
       "  [0, 9],\n",
       "  [0, 10],\n",
       "  [0, 11],\n",
       "  [0, 12],\n",
       "  [0, 13],\n",
       "  [0, 14],\n",
       "  [0, 15],\n",
       "  [0, 16],\n",
       "  [0, 17],\n",
       "  [0, 18],\n",
       "  [0, 19],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [1, 4],\n",
       "  [1, 5],\n",
       "  [1, 6],\n",
       "  [1, 7],\n",
       "  [1, 8],\n",
       "  [1, 9],\n",
       "  [1, 10],\n",
       "  [1, 11],\n",
       "  [1, 12],\n",
       "  [1, 13],\n",
       "  [1, 14],\n",
       "  [1, 15],\n",
       "  [1, 16],\n",
       "  [1, 17],\n",
       "  [1, 18],\n",
       "  [1, 19],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2, 2],\n",
       "  [2, 3],\n",
       "  [2, 4],\n",
       "  [2, 5],\n",
       "  [2, 6],\n",
       "  [2, 7],\n",
       "  [2, 8],\n",
       "  [2, 9],\n",
       "  [2, 10],\n",
       "  [2, 11],\n",
       "  [2, 12],\n",
       "  [2, 13],\n",
       "  [2, 14],\n",
       "  [2, 15],\n",
       "  [2, 16],\n",
       "  [2, 17],\n",
       "  [2, 18],\n",
       "  [2, 19],\n",
       "  [3, 0],\n",
       "  [3, 1],\n",
       "  [3, 2],\n",
       "  [3, 3],\n",
       "  [3, 4],\n",
       "  [3, 5],\n",
       "  [3, 6],\n",
       "  [3, 7],\n",
       "  [3, 8],\n",
       "  [3, 9],\n",
       "  [3, 10],\n",
       "  [3, 11],\n",
       "  [3, 12],\n",
       "  [3, 13],\n",
       "  [3, 14],\n",
       "  [3, 15],\n",
       "  [3, 16],\n",
       "  [3, 17],\n",
       "  [3, 18],\n",
       "  [3, 19],\n",
       "  [4, 0],\n",
       "  [4, 1],\n",
       "  [4, 2],\n",
       "  [4, 3],\n",
       "  [4, 4],\n",
       "  [4, 5],\n",
       "  [4, 6],\n",
       "  [4, 7],\n",
       "  [4, 8],\n",
       "  [4, 9],\n",
       "  [4, 10],\n",
       "  [4, 11],\n",
       "  [4, 12],\n",
       "  [4, 13],\n",
       "  [4, 14],\n",
       "  [4, 15],\n",
       "  [4, 16],\n",
       "  [4, 17],\n",
       "  [4, 18],\n",
       "  [4, 19],\n",
       "  [5, 0],\n",
       "  [5, 1],\n",
       "  [5, 2],\n",
       "  [5, 3],\n",
       "  [5, 4],\n",
       "  [5, 5],\n",
       "  [5, 6],\n",
       "  [5, 7],\n",
       "  [5, 8],\n",
       "  [5, 9],\n",
       "  [5, 10],\n",
       "  [5, 11],\n",
       "  [5, 12],\n",
       "  [5, 13],\n",
       "  [5, 14],\n",
       "  [5, 15],\n",
       "  [5, 16],\n",
       "  [5, 17],\n",
       "  [5, 18],\n",
       "  [5, 19],\n",
       "  [6, 0],\n",
       "  [6, 1],\n",
       "  [6, 2],\n",
       "  [6, 3],\n",
       "  [6, 4],\n",
       "  [6, 5],\n",
       "  [6, 6],\n",
       "  [6, 7],\n",
       "  [6, 8],\n",
       "  [6, 9],\n",
       "  [6, 10],\n",
       "  [6, 11],\n",
       "  [6, 12],\n",
       "  [6, 13],\n",
       "  [6, 14],\n",
       "  [6, 15],\n",
       "  [6, 16],\n",
       "  [6, 17],\n",
       "  [6, 18],\n",
       "  [6, 19],\n",
       "  [7, 0],\n",
       "  [7, 1],\n",
       "  [7, 2],\n",
       "  [7, 3],\n",
       "  [7, 4],\n",
       "  [7, 5],\n",
       "  [7, 6],\n",
       "  [7, 7],\n",
       "  [7, 8],\n",
       "  [7, 9],\n",
       "  [7, 10],\n",
       "  [7, 11],\n",
       "  [7, 12],\n",
       "  [7, 13],\n",
       "  [7, 14],\n",
       "  [7, 15],\n",
       "  [7, 16],\n",
       "  [7, 17],\n",
       "  [7, 18],\n",
       "  [7, 19],\n",
       "  [8, 0],\n",
       "  [8, 1],\n",
       "  [8, 2],\n",
       "  [8, 3],\n",
       "  [8, 4],\n",
       "  [8, 5],\n",
       "  [8, 6],\n",
       "  [8, 7],\n",
       "  [8, 8],\n",
       "  [8, 9],\n",
       "  [8, 10],\n",
       "  [8, 11],\n",
       "  [8, 12],\n",
       "  [8, 13],\n",
       "  [8, 14],\n",
       "  [8, 15],\n",
       "  [8, 16],\n",
       "  [8, 17],\n",
       "  [8, 18],\n",
       "  [8, 19],\n",
       "  [9, 0],\n",
       "  [9, 1],\n",
       "  [9, 2],\n",
       "  [9, 3],\n",
       "  [9, 4],\n",
       "  [9, 5],\n",
       "  [9, 6],\n",
       "  [9, 7],\n",
       "  [9, 8],\n",
       "  [9, 9],\n",
       "  [9, 10],\n",
       "  [9, 11],\n",
       "  [9, 12],\n",
       "  [9, 13],\n",
       "  [9, 14],\n",
       "  [9, 15],\n",
       "  [9, 16],\n",
       "  [9, 17],\n",
       "  [9, 18],\n",
       "  [9, 19],\n",
       "  [10, 0],\n",
       "  [10, 1],\n",
       "  [10, 2],\n",
       "  [10, 3],\n",
       "  [10, 4],\n",
       "  [10, 5],\n",
       "  [10, 6],\n",
       "  [10, 7],\n",
       "  [10, 8],\n",
       "  [10, 9],\n",
       "  [10, 10],\n",
       "  [10, 11],\n",
       "  [10, 12],\n",
       "  [10, 13],\n",
       "  [10, 14],\n",
       "  [10, 15],\n",
       "  [10, 16],\n",
       "  [10, 17],\n",
       "  [10, 18],\n",
       "  [10, 19],\n",
       "  [11, 0],\n",
       "  [11, 1],\n",
       "  [11, 2],\n",
       "  [11, 3],\n",
       "  [11, 4],\n",
       "  [11, 5],\n",
       "  [11, 6],\n",
       "  [11, 7],\n",
       "  [11, 8],\n",
       "  [11, 9],\n",
       "  [11, 10],\n",
       "  [11, 11],\n",
       "  [11, 12],\n",
       "  [11, 13],\n",
       "  [11, 14],\n",
       "  [11, 15],\n",
       "  [11, 16],\n",
       "  [11, 17],\n",
       "  [11, 18],\n",
       "  [11, 19],\n",
       "  [12, 0],\n",
       "  [12, 1],\n",
       "  [12, 2],\n",
       "  [12, 3],\n",
       "  [12, 4],\n",
       "  [12, 5],\n",
       "  [12, 6],\n",
       "  [12, 7],\n",
       "  [12, 8],\n",
       "  [12, 9],\n",
       "  [12, 10],\n",
       "  [12, 11],\n",
       "  [12, 12],\n",
       "  [12, 13],\n",
       "  [12, 14],\n",
       "  [12, 15],\n",
       "  [12, 16],\n",
       "  [12, 17],\n",
       "  [12, 18],\n",
       "  [12, 19],\n",
       "  [13, 0],\n",
       "  [13, 1],\n",
       "  [13, 2],\n",
       "  [13, 3],\n",
       "  [13, 4],\n",
       "  [13, 5],\n",
       "  [13, 6],\n",
       "  [13, 7],\n",
       "  [13, 8],\n",
       "  [13, 9],\n",
       "  [13, 10],\n",
       "  [13, 11],\n",
       "  [13, 12],\n",
       "  [13, 13],\n",
       "  [13, 14],\n",
       "  [13, 15],\n",
       "  [13, 16],\n",
       "  [13, 17],\n",
       "  [13, 18],\n",
       "  [13, 19],\n",
       "  [14, 0],\n",
       "  [14, 1],\n",
       "  [14, 2],\n",
       "  [14, 3],\n",
       "  [14, 4],\n",
       "  [14, 5],\n",
       "  [14, 6],\n",
       "  [14, 7],\n",
       "  [14, 8],\n",
       "  [14, 9],\n",
       "  [14, 10],\n",
       "  [14, 11],\n",
       "  [14, 12],\n",
       "  [14, 13],\n",
       "  [14, 14],\n",
       "  [14, 15],\n",
       "  [14, 16],\n",
       "  [14, 17],\n",
       "  [14, 18],\n",
       "  [14, 19],\n",
       "  [15, 0],\n",
       "  [15, 1],\n",
       "  [15, 2],\n",
       "  [15, 3],\n",
       "  [15, 4],\n",
       "  [15, 5],\n",
       "  [15, 6],\n",
       "  [15, 7],\n",
       "  [15, 8],\n",
       "  [15, 9],\n",
       "  [15, 10],\n",
       "  [15, 11],\n",
       "  [15, 12],\n",
       "  [15, 13],\n",
       "  [15, 14],\n",
       "  [15, 15],\n",
       "  [15, 16],\n",
       "  [15, 17],\n",
       "  [15, 18],\n",
       "  [15, 19],\n",
       "  [16, 0],\n",
       "  [16, 1],\n",
       "  [16, 2],\n",
       "  [16, 3],\n",
       "  [16, 4],\n",
       "  [16, 5],\n",
       "  [16, 6],\n",
       "  [16, 7],\n",
       "  [16, 8],\n",
       "  [16, 9],\n",
       "  [16, 10],\n",
       "  [16, 11],\n",
       "  [16, 12],\n",
       "  [16, 13],\n",
       "  [16, 14],\n",
       "  [16, 15],\n",
       "  [16, 16],\n",
       "  [16, 17],\n",
       "  [16, 18],\n",
       "  [16, 19],\n",
       "  [17, 0],\n",
       "  [17, 1],\n",
       "  [17, 2],\n",
       "  [17, 3],\n",
       "  [17, 4],\n",
       "  [17, 5],\n",
       "  [17, 6],\n",
       "  [17, 7],\n",
       "  [17, 8],\n",
       "  [17, 9],\n",
       "  [17, 10],\n",
       "  [17, 11],\n",
       "  [17, 12],\n",
       "  [17, 13],\n",
       "  [17, 14],\n",
       "  [17, 15],\n",
       "  [17, 16],\n",
       "  [17, 17],\n",
       "  [17, 18],\n",
       "  [17, 19],\n",
       "  [18, 0],\n",
       "  [18, 1],\n",
       "  [18, 2],\n",
       "  [18, 3],\n",
       "  [18, 4],\n",
       "  [18, 5],\n",
       "  [18, 6],\n",
       "  [18, 7],\n",
       "  [18, 8],\n",
       "  [18, 9],\n",
       "  [18, 10],\n",
       "  [18, 11],\n",
       "  [18, 12],\n",
       "  [18, 13],\n",
       "  [18, 14],\n",
       "  [18, 15],\n",
       "  [18, 16],\n",
       "  [18, 17],\n",
       "  [18, 18],\n",
       "  [18, 19],\n",
       "  [19, 0],\n",
       "  [19, 1],\n",
       "  [19, 2],\n",
       "  [19, 3],\n",
       "  [19, 4],\n",
       "  [19, 5],\n",
       "  [19, 6],\n",
       "  [19, 7],\n",
       "  [19, 8],\n",
       "  [19, 9],\n",
       "  [19, 10],\n",
       "  [19, 11],\n",
       "  [19, 12],\n",
       "  [19, 13],\n",
       "  [19, 14],\n",
       "  [19, 15],\n",
       "  [19, 16],\n",
       "  [19, 17],\n",
       "  [19, 18],\n",
       "  [19, 19]],\n",
       " 398)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.free_spaces(), len(board.free_spaces())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5055d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn((1, 2, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468a39d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.softmax(-1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a76a9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f2bbcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76c1cdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [0],\n",
       "         [2]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.multinomial(a, num_samples=1)\n",
    "b, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f43ce9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [4.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(a, 1, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6638326",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_data = {'Root': \n",
    "            [\n",
    "                {'3/0': \n",
    "                 [{'2/0': \n",
    "                   [{'1/0': [0.541657030582428]}, {'1/1': [0.4583829641342163]}]},\n",
    "                 {'2/1': \n",
    "                  [{'1/0': [0.4701797366142273]}, {'1/1': [0.4208185076713562]}]}]}, \n",
    "                {'3/1': \n",
    "                 [{'2/0': \n",
    "                   [{'1/0': [0.4985354542732239]}]}, \n",
    "                 {'2/1': \n",
    "                   [{'1/0': [0.463384747505188]}, {'1/1': [0.5391099452972412]}]}]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5ec32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2abb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b15249b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHILD {'3/0': [{'2/0': [{'1/0': [0.541657030582428]}, {'1/1': [0.4583829641342163]}]}, {'2/1': [{'1/0': [0.4701797366142273]}, {'1/1': [0.4208185076713562]}]}]}\n",
      "CHILD {'2/0': [{'1/0': [0.541657030582428]}, {'1/1': [0.4583829641342163]}]}\n",
      "CHILD {'1/0': [0.541657030582428]}\n",
      "CHILD 0.541657030582428\n",
      "Error 0.541657030582428\n",
      "CHILD {'1/1': [0.4583829641342163]}\n",
      "CHILD 0.4583829641342163\n",
      "Error 0.4583829641342163\n",
      "CHILD {'2/1': [{'1/0': [0.4701797366142273]}, {'1/1': [0.4208185076713562]}]}\n",
      "CHILD {'1/0': [0.4701797366142273]}\n",
      "CHILD 0.4701797366142273\n",
      "Error 0.4701797366142273\n",
      "CHILD {'1/1': [0.4208185076713562]}\n",
      "CHILD 0.4208185076713562\n",
      "Error 0.4208185076713562\n",
      "CHILD {'3/1': [{'2/0': [{'1/0': [0.4985354542732239]}]}, {'2/1': [{'1/0': [0.463384747505188]}, {'1/1': [0.5391099452972412]}]}]}\n",
      "CHILD {'2/0': [{'1/0': [0.4985354542732239]}]}\n",
      "CHILD {'1/0': [0.4985354542732239]}\n",
      "CHILD 0.4985354542732239\n",
      "Error 0.4985354542732239\n",
      "CHILD {'2/1': [{'1/0': [0.463384747505188]}, {'1/1': [0.5391099452972412]}]}\n",
      "CHILD {'1/0': [0.463384747505188]}\n",
      "CHILD 0.463384747505188\n",
      "Error 0.463384747505188\n",
      "CHILD {'1/1': [0.5391099452972412]}\n",
      "CHILD 0.5391099452972412\n",
      "Error 0.5391099452972412\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "graph = graphviz.Digraph()\n",
    "\n",
    "def get(data_dict, graph, parent_id=None, last=False):  \n",
    "    if last:\n",
    "        my_id = str(uuid.uuid4())\n",
    "        print(\"Error\", data_dict)\n",
    "        graph.node(my_id, str(data_dict))\n",
    "        graph.edge(my_id, parent_id)\n",
    "        return\n",
    "\n",
    "    for data in data_dict.keys():\n",
    "        my_id = str(uuid.uuid4())\n",
    "        graph.node(my_id, data)\n",
    "\n",
    "        if parent_id != None:\n",
    "            graph.edge(my_id, parent_id)\n",
    "\n",
    "        for child in data_dict[data]:\n",
    "            print(\"CHILD\", child)\n",
    "            get(data_dict=child, graph=graph, parent_id=my_id, last=type(child)==type(1.2))\n",
    "                \n",
    "get(viz_data, graph, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f75254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dt_graph = graphviz.Source(viz_data, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29fe207d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1325pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 1324.93 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-328 1320.93,-328 1320.93,4 -4,4\"/>\n",
       "<!-- fb4d4e3d&#45;9887&#45;4f0c&#45;a8fb&#45;66fc40d487fa -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>fb4d4e3d&#45;9887&#45;4f0c&#45;a8fb&#45;66fc40d487fa</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"754.19\" cy=\"-18\" rx=\"27.9\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"754.19\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Root</text>\n",
       "</g>\n",
       "<!-- 72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"561.19\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"561.19\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">3/0</text>\n",
       "</g>\n",
       "<!-- 72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16&#45;&gt;fb4d4e3d&#45;9887&#45;4f0c&#45;a8fb&#45;66fc40d487fa -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16&#45;&gt;fb4d4e3d&#45;9887&#45;4f0c&#45;a8fb&#45;66fc40d487fa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M584.47,-80.56C618.43,-68.24 681.74,-45.28 720.7,-31.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"722.32,-34.28 730.53,-27.58 719.93,-27.7 722.32,-34.28\"/>\n",
       "</g>\n",
       "<!-- 3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"272.19\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.19\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">2/0</text>\n",
       "</g>\n",
       "<!-- 3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456&#45;&gt;72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456&#45;&gt;72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M297.08,-154.97C348.62,-142.49 467.12,-113.79 526.27,-99.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"527.36,-102.79 536.26,-97.04 525.72,-95.99 527.36,-102.79\"/>\n",
       "</g>\n",
       "<!-- bbeea06f&#45;a461&#45;4839&#45;a260&#45;07e425cd5447 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>bbeea06f&#45;a461&#45;4839&#45;a260&#45;07e425cd5447</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"142.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"142.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/0</text>\n",
       "</g>\n",
       "<!-- bbeea06f&#45;a461&#45;4839&#45;a260&#45;07e425cd5447&#45;&gt;3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>bbeea06f&#45;a461&#45;4839&#45;a260&#45;07e425cd5447&#45;&gt;3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.51,-222.06C184.03,-210.47 218.24,-192.05 242.73,-178.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.45,-181.91 251.59,-174.09 241.13,-175.75 244.45,-181.91\"/>\n",
       "</g>\n",
       "<!-- d7c4a184&#45;8a39&#45;4766&#45;b60e&#45;90eaf349a116 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>d7c4a184&#45;8a39&#45;4766&#45;b60e&#45;90eaf349a116</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"83.19\" cy=\"-306\" rx=\"83.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.541657030582428</text>\n",
       "</g>\n",
       "<!-- d7c4a184&#45;8a39&#45;4766&#45;b60e&#45;90eaf349a116&#45;&gt;bbeea06f&#45;a461&#45;4839&#45;a260&#45;07e425cd5447 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>d7c4a184&#45;8a39&#45;4766&#45;b60e&#45;90eaf349a116&#45;&gt;bbeea06f&#45;a461&#45;4839&#45;a260&#45;07e425cd5447</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M97.47,-288.05C105.04,-279.08 114.45,-267.92 122.68,-258.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.48,-260.26 129.25,-250.36 120.13,-255.75 125.48,-260.26\"/>\n",
       "</g>\n",
       "<!-- b7d9dacf&#45;7e9c&#45;4efe&#45;a9d7&#45;131b3078d5e4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>b7d9dacf&#45;7e9c&#45;4efe&#45;a9d7&#45;131b3078d5e4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"272.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- b7d9dacf&#45;7e9c&#45;4efe&#45;a9d7&#45;131b3078d5e4&#45;&gt;3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>b7d9dacf&#45;7e9c&#45;4efe&#45;a9d7&#45;131b3078d5e4&#45;&gt;3d232808&#45;7eb5&#45;4e90&#45;b7aa&#45;21a0a9cc0456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M272.19,-215.7C272.19,-207.98 272.19,-198.71 272.19,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275.69,-190.1 272.19,-180.1 268.69,-190.1 275.69,-190.1\"/>\n",
       "</g>\n",
       "<!-- 6ce111dc&#45;f289&#45;466c&#45;9f5d&#45;a41621616b28 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6ce111dc&#45;f289&#45;466c&#45;9f5d&#45;a41621616b28</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"272.19\" cy=\"-306\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.4583829641342163</text>\n",
       "</g>\n",
       "<!-- 6ce111dc&#45;f289&#45;466c&#45;9f5d&#45;a41621616b28&#45;&gt;b7d9dacf&#45;7e9c&#45;4efe&#45;a9d7&#45;131b3078d5e4 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6ce111dc&#45;f289&#45;466c&#45;9f5d&#45;a41621616b28&#45;&gt;b7d9dacf&#45;7e9c&#45;4efe&#45;a9d7&#45;131b3078d5e4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M272.19,-287.7C272.19,-279.98 272.19,-270.71 272.19,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275.69,-262.1 272.19,-252.1 268.69,-262.1 275.69,-262.1\"/>\n",
       "</g>\n",
       "<!-- 6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"561.19\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"561.19\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">2/1</text>\n",
       "</g>\n",
       "<!-- 6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f&#45;&gt;72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f&#45;&gt;72deaecf&#45;0a44&#45;42a5&#45;93d6&#45;7f7a19951f16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M561.19,-143.7C561.19,-135.98 561.19,-126.71 561.19,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"564.69,-118.1 561.19,-108.1 557.69,-118.1 564.69,-118.1\"/>\n",
       "</g>\n",
       "<!-- 2db4deaa&#45;ec9a&#45;4851&#45;bd18&#45;4c23af8ec83e -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2db4deaa&#45;ec9a&#45;4851&#45;bd18&#45;4c23af8ec83e</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"495.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"495.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/0</text>\n",
       "</g>\n",
       "<!-- 2db4deaa&#45;ec9a&#45;4851&#45;bd18&#45;4c23af8ec83e&#45;&gt;6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2db4deaa&#45;ec9a&#45;4851&#45;bd18&#45;4c23af8ec83e&#45;&gt;6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M509.19,-218.15C518.2,-208.6 530.08,-195.99 540.21,-185.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"542.9,-187.5 547.22,-177.82 537.81,-182.7 542.9,-187.5\"/>\n",
       "</g>\n",
       "<!-- c7e4e2a3&#45;3c08&#45;4072&#45;9f8b&#45;59fdd3762437 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>c7e4e2a3&#45;3c08&#45;4072&#45;9f8b&#45;59fdd3762437</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"465.19\" cy=\"-306\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"465.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.4701797366142273</text>\n",
       "</g>\n",
       "<!-- c7e4e2a3&#45;3c08&#45;4072&#45;9f8b&#45;59fdd3762437&#45;&gt;2db4deaa&#45;ec9a&#45;4851&#45;bd18&#45;4c23af8ec83e -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>c7e4e2a3&#45;3c08&#45;4072&#45;9f8b&#45;59fdd3762437&#45;&gt;2db4deaa&#45;ec9a&#45;4851&#45;bd18&#45;4c23af8ec83e</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.45,-288.05C475.92,-279.97 480.14,-270.12 484,-261.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"487.28,-262.36 488,-251.79 480.84,-259.6 487.28,-262.36\"/>\n",
       "</g>\n",
       "<!-- 29da2b2f&#45;a7fb&#45;410e&#45;8f0a&#45;325fda5c0b8a -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>29da2b2f&#45;a7fb&#45;410e&#45;8f0a&#45;325fda5c0b8a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"598.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"598.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- 29da2b2f&#45;a7fb&#45;410e&#45;8f0a&#45;325fda5c0b8a&#45;&gt;6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>29da2b2f&#45;a7fb&#45;410e&#45;8f0a&#45;325fda5c0b8a&#45;&gt;6ae02bfb&#45;de06&#45;48de&#45;b92a&#45;a4eb3e306b7f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M589.61,-216.76C585.13,-208.28 579.54,-197.71 574.51,-188.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"577.5,-186.35 569.73,-179.15 571.31,-189.62 577.5,-186.35\"/>\n",
       "</g>\n",
       "<!-- d71be8d7&#45;a719&#45;4114&#45;8616&#45;65d5c1285da9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>d71be8d7&#45;a719&#45;4114&#45;8616&#45;65d5c1285da9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"658.19\" cy=\"-306\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"658.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.4208185076713562</text>\n",
       "</g>\n",
       "<!-- d71be8d7&#45;a719&#45;4114&#45;8616&#45;65d5c1285da9&#45;&gt;29da2b2f&#45;a7fb&#45;410e&#45;8f0a&#45;325fda5c0b8a -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>d71be8d7&#45;a719&#45;4114&#45;8616&#45;65d5c1285da9&#45;&gt;29da2b2f&#45;a7fb&#45;410e&#45;8f0a&#45;325fda5c0b8a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M643.67,-288.05C635.86,-278.95 626.14,-267.6 617.68,-257.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"620.12,-255.2 610.95,-249.89 614.8,-259.76 620.12,-255.2\"/>\n",
       "</g>\n",
       "<!-- d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"851.19\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"851.19\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">3/1</text>\n",
       "</g>\n",
       "<!-- d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191&#45;&gt;fb4d4e3d&#45;9887&#45;4f0c&#45;a8fb&#45;66fc40d487fa -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191&#45;&gt;fb4d4e3d&#45;9887&#45;4f0c&#45;a8fb&#45;66fc40d487fa</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M833.4,-76.16C818.59,-65.47 797.29,-50.1 780.51,-37.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"782.36,-35.01 772.2,-31.99 778.26,-40.68 782.36,-35.01\"/>\n",
       "</g>\n",
       "<!-- 5ae01ebb&#45;c9fb&#45;4ad3&#45;b4a1&#45;3b8a7bf7043c -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>5ae01ebb&#45;c9fb&#45;4ad3&#45;b4a1&#45;3b8a7bf7043c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"851.19\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"851.19\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">2/0</text>\n",
       "</g>\n",
       "<!-- 5ae01ebb&#45;c9fb&#45;4ad3&#45;b4a1&#45;3b8a7bf7043c&#45;&gt;d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>5ae01ebb&#45;c9fb&#45;4ad3&#45;b4a1&#45;3b8a7bf7043c&#45;&gt;d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M851.19,-143.7C851.19,-135.98 851.19,-126.71 851.19,-118.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"854.69,-118.1 851.19,-108.1 847.69,-118.1 854.69,-118.1\"/>\n",
       "</g>\n",
       "<!-- 1c71a4f1&#45;b848&#45;48af&#45;a7f0&#45;1c9bcab83425 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>1c71a4f1&#45;b848&#45;48af&#45;a7f0&#45;1c9bcab83425</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"851.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"851.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/0</text>\n",
       "</g>\n",
       "<!-- 1c71a4f1&#45;b848&#45;48af&#45;a7f0&#45;1c9bcab83425&#45;&gt;5ae01ebb&#45;c9fb&#45;4ad3&#45;b4a1&#45;3b8a7bf7043c -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>1c71a4f1&#45;b848&#45;48af&#45;a7f0&#45;1c9bcab83425&#45;&gt;5ae01ebb&#45;c9fb&#45;4ad3&#45;b4a1&#45;3b8a7bf7043c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M851.19,-215.7C851.19,-207.98 851.19,-198.71 851.19,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"854.69,-190.1 851.19,-180.1 847.69,-190.1 854.69,-190.1\"/>\n",
       "</g>\n",
       "<!-- e4e4624e&#45;9d2f&#45;447d&#45;87df&#45;d702f72d3154 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>e4e4624e&#45;9d2f&#45;447d&#45;87df&#45;d702f72d3154</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"851.19\" cy=\"-306\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"851.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.4985354542732239</text>\n",
       "</g>\n",
       "<!-- e4e4624e&#45;9d2f&#45;447d&#45;87df&#45;d702f72d3154&#45;&gt;1c71a4f1&#45;b848&#45;48af&#45;a7f0&#45;1c9bcab83425 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>e4e4624e&#45;9d2f&#45;447d&#45;87df&#45;d702f72d3154&#45;&gt;1c71a4f1&#45;b848&#45;48af&#45;a7f0&#45;1c9bcab83425</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M851.19,-287.7C851.19,-279.98 851.19,-270.71 851.19,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"854.69,-262.1 851.19,-252.1 847.69,-262.1 854.69,-262.1\"/>\n",
       "</g>\n",
       "<!-- 3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1040.19\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1040.19\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">2/1</text>\n",
       "</g>\n",
       "<!-- 3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950&#45;&gt;d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950&#45;&gt;d54038b6&#45;9110&#45;4294&#45;8821&#45;d77a528a8191</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1017.02,-152.42C983.63,-140.05 921.84,-117.17 883.84,-103.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.85,-99.74 874.26,-99.54 882.42,-106.3 884.85,-99.74\"/>\n",
       "</g>\n",
       "<!-- 1669eedf&#45;4d03&#45;4545&#45;a30a&#45;5a2384c2e785 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>1669eedf&#45;4d03&#45;4545&#45;a30a&#45;5a2384c2e785</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1040.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1040.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/0</text>\n",
       "</g>\n",
       "<!-- 1669eedf&#45;4d03&#45;4545&#45;a30a&#45;5a2384c2e785&#45;&gt;3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>1669eedf&#45;4d03&#45;4545&#45;a30a&#45;5a2384c2e785&#45;&gt;3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1040.19,-215.7C1040.19,-207.98 1040.19,-198.71 1040.19,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1043.69,-190.1 1040.19,-180.1 1036.69,-190.1 1043.69,-190.1\"/>\n",
       "</g>\n",
       "<!-- 9c1bb42f&#45;8dd9&#45;47e2&#45;9bca&#45;dca8770c7ea0 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>9c1bb42f&#45;8dd9&#45;47e2&#45;9bca&#45;dca8770c7ea0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1040.19\" cy=\"-306\" rx=\"83.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1040.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.463384747505188</text>\n",
       "</g>\n",
       "<!-- 9c1bb42f&#45;8dd9&#45;47e2&#45;9bca&#45;dca8770c7ea0&#45;&gt;1669eedf&#45;4d03&#45;4545&#45;a30a&#45;5a2384c2e785 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>9c1bb42f&#45;8dd9&#45;47e2&#45;9bca&#45;dca8770c7ea0&#45;&gt;1669eedf&#45;4d03&#45;4545&#45;a30a&#45;5a2384c2e785</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1040.19,-287.7C1040.19,-279.98 1040.19,-270.71 1040.19,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1043.69,-262.1 1040.19,-252.1 1036.69,-262.1 1043.69,-262.1\"/>\n",
       "</g>\n",
       "<!-- c0439c2e&#45;00c0&#45;4c1a&#45;8039&#45;c96345c519b4 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>c0439c2e&#45;00c0&#45;4c1a&#45;8039&#45;c96345c519b4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1170.19\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1170.19\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- c0439c2e&#45;00c0&#45;4c1a&#45;8039&#45;c96345c519b4&#45;&gt;3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>c0439c2e&#45;00c0&#45;4c1a&#45;8039&#45;c96345c519b4&#45;&gt;3c7930f3&#45;88b7&#45;4a8d&#45;8b4a&#45;fd308c5d2950</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1149.88,-222.06C1128.35,-210.47 1094.14,-192.05 1069.65,-178.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1071.26,-175.75 1060.79,-174.09 1067.94,-181.91 1071.26,-175.75\"/>\n",
       "</g>\n",
       "<!-- 5ed341c7&#45;a1f6&#45;481d&#45;be84&#45;0b8c5b635106 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>5ed341c7&#45;a1f6&#45;481d&#45;be84&#45;0b8c5b635106</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1229.19\" cy=\"-306\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1229.19\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.5391099452972412</text>\n",
       "</g>\n",
       "<!-- 5ed341c7&#45;a1f6&#45;481d&#45;be84&#45;0b8c5b635106&#45;&gt;c0439c2e&#45;00c0&#45;4c1a&#45;8039&#45;c96345c519b4 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>5ed341c7&#45;a1f6&#45;481d&#45;be84&#45;0b8c5b635106&#45;&gt;c0439c2e&#45;00c0&#45;4c1a&#45;8039&#45;c96345c519b4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1214.91,-288.05C1207.34,-279.08 1197.94,-267.92 1189.71,-258.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1192.26,-255.75 1183.14,-250.36 1186.9,-260.26 1192.26,-255.75\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fd113ced9c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5ca59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467d2809-e314-407c-8941-3aa038e6bddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcd0d514-88f4-4751-bd00-bd8d58e55d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 6],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4,5, 6]])\n",
    "print(a)\n",
    "\n",
    "torch.roll(a, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20002eb-fb18-493c-b695-c62af3957927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 601.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from train_base import GOMUDataset\n",
    "dataset = GOMUDataset(2, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7473f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, result = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b157a60b-0966-4764-b0ab-207adf183e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d9fc7e-9148-4f6e-965d-0d3d336f9e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (118933863.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    x.shape, x[0]-x[1]|\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x.shape, x[0]-x[1]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd69b7b-be52-40e4-8731-b0f5abca7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAAEzElEQVR4Ae3cwW0CMRAFUJIiUJoLVZAqQhVLcxFVkBOSL2Bbaw0ev5yyYtf2vK8vLisOB38ECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQGBygY99z3/7u71c8PxzfnnP7jds1233NS2YQOAzwQxGINAtoADddB7MIKAAGVI0Q7eAAnTTeTCDgAJkSNEM3QIK0E3nwQwCCpAhRTN0CyhAN50HMwgoQIYUzdAtoADddB7MIKAAGVI0Q7eAAnTTeZAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIE3kWg9oexan7x6n+mmh+9uvxedpz++HXccTVLrSbgbdDVEjdvIaAABYeL1QQUYLXEzVsIKEDB4WI1AQVYLXHzFgIKUHC4WE1AAVZL3LyFgAIUHC5WE1CA1RI3byGgAAWHi9UEFGC1xM1bCChAweFiNYHal+He1qXyLT3vzL1tgrEH8w0Q62/3YAEFCA7A9rECChDrb/dgAQUIDsD2sQIKEOtv92ABBQgOwPaxAgoQ62/3YAEFCA7A9rECChDrb/dgAQUIDsD2sQIKEOtv92ABBQgOwPaxArUvw3nnLDYnuw8S8A0wCNaycwgowBw5OeUgAQUYBGvZOQQUYI6cnHKQgAIMgrXsHAIKMEdOTjlIQAEGwVp2DgEFmCMnpxwkoACDYC07h4ACzJGTUw4SUIBBsJadQ0AB5sjJKQcJ1L4MN2j7J8uevk9PPn18tF23x//+IdAq4BugVcz9qQQUIFWchmkVUIBWMfenElCAVHEaplVAAVrF3J9KQAFSxWmYVgEFaBVzfyoBBUgVp2FaBRSgVcz9qQQUIFWchmkVUIBWMfenElCAVHEahgABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEFhX4A7QnxV+X8ISKAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gomu.gomuku.board import GoMuKuBoard\n",
    "GoMuKuBoard.viz(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8658ae64-8263-4fb7-bd92-46271a403122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAADrklEQVR4Ae3T0QmAMBAD0Oqedri6qCsUAmrg9bvhjndkDI8AAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEvhU43h8/r7kzdN1r55s/BBKBMwnLEmgXUID2C9o/ElCAiE+4XUAB2i9o/0hAASI+4XYBBWi/oP0jAQWI+ITbBRSg/YL2jwQUIOITbhdQgPYL2j8SUICIT5gAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgX8IPBAOAxjbgYm1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gomu.viz import tensor2gomuboard\n",
    "tensor2gomuboard(y, nrow=20, ncol=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3719213c-6170-409a-a0ae-25e89664760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbbd24f-ec5b-4832-884b-c9dc7ba319e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[0, 1, 1], [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790f5fc6-80e9-4760-8842-99a062b8166a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ef24e47-20f5-494a-9d0c-9398ba5d1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros((2, 1, 2)).repeat(12, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c73c399-a2ba-4a07-bd1c-9586eb368c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "a[1][[[1, 2], [1, 3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c3d12d-6bd5-45cd-91d7-3b1840234e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2d990c-7a3b-4111-8720-09739e9bc5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1, 1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945a976a-30c7-44e5-8ce1-a7fe09f34b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gomu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119cc750-45ae-41fc-8fda-5b3e5e2ef8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gomu.base in gomu:\n",
      "\n",
      "NAME\n",
      "    gomu.base\n",
      "\n",
      "CLASSES\n",
      "    torch.nn.modules.module.Module(builtins.object)\n",
      "        Attention\n",
      "        FeedForward\n",
      "        MBConv\n",
      "        PocliyValueNet\n",
      "        PreNorm\n",
      "        ResBlock\n",
      "        SE\n",
      "        Transformer\n",
      "        Unet\n",
      "    \n",
      "    class Attention(torch.nn.modules.module.Module)\n",
      "     |  Attention(inp, oup, image_size, heads=8, dim_head=32, dropout=0.0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Attention\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.0)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class FeedForward(torch.nn.modules.module.Module)\n",
      "     |  FeedForward(dim, hidden_dim, dropout=0.0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FeedForward\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dim, hidden_dim, dropout=0.0)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MBConv(torch.nn.modules.module.Module)\n",
      "     |  MBConv(inp, oup, downsample=False, **kwargs)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MBConv\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inp, oup, downsample=False, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x, xt=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PocliyValueNet(torch.nn.modules.module.Module)\n",
      "     |  PocliyValueNet(nrow, ncol, channels, dropout)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PocliyValueNet\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nrow, ncol, channels, dropout)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PreNorm(torch.nn.modules.module.Module)\n",
      "     |  PreNorm(dim, fn, norm)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PreNorm\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dim, fn, norm)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x, **kwargs)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ResBlock(torch.nn.modules.module.Module)\n",
      "     |  ResBlock(inp, mid, oup, stride=1, dropout=0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ResBlock\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inp, mid, oup, stride=1, dropout=0)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class SE(torch.nn.modules.module.Module)\n",
      "     |  SE(inp, oup, expansion=0.25)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SE\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inp, oup, expansion=0.25)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class Transformer(torch.nn.modules.module.Module)\n",
      "     |  Transformer(inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Transformer\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.0)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class Unet(torch.nn.modules.module.Module)\n",
      "     |  Unet(nrow, ncol, channels)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Unet\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nrow, ncol, channels)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, x)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Any\n",
      "     |      # On the return type:\n",
      "     |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      "     |      # This is done for better interop with various type checkers for the end users.\n",
      "     |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      "     |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      "     |      # See full discussion on the problems with returning `Union` here\n",
      "     |      # https://github.com/microsoft/pyright/issues/4213\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[1., 1.],\n",
      "     |                  [1., 1.]], requires_grad=True)\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  compile(self, *args, **kwargs)\n",
      "     |      Compile this Module's forward using :func:`torch.compile`.\n",
      "     |      \n",
      "     |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      "     |      to :func:`torch.compile`.\n",
      "     |      \n",
      "     |      See :func:`torch.compile` for details on the arguments for this function.\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target: str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be picklable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target: str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target: str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block:: text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the IPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on IPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      "     |          the call to :attr:`load_state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |          assign (bool, optional): whether to assign items in the state\n",
      "     |              dictionary to their corresponding keys in the module instead\n",
      "     |              of copying them inplace into the module's current parameters and buffers.\n",
      "     |              When ``False``, the properties of the tensors in the current\n",
      "     |              module are preserved while when ``True``, the properties of the\n",
      "     |              Tensors in the state dict are preserved.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool, optional): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module. Defaults to True.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>     if name in ['running_var']:\n",
      "     |          >>>         print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |              or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |          ...     print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      "     |              parameters in the result. Defaults to True.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (str, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>     if name in ['bias']:\n",
      "     |          >>>         print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      output. It can modify the input inplace but it will not have effect on\n",
      "     |      forward since this is called after :func:`forward` is called. The hook\n",
      "     |      should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, output) -> None or modified output\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      "     |      ``kwargs`` given to the forward function and be expected to return the\n",
      "     |      output possibly modified. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs, output) -> None or modified output\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      "     |              before all existing ``forward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward`` hooks registered with\n",
      "     |              :func:`register_module_forward_hook` will fire before all hooks\n",
      "     |              registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      "     |              kwargs given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      "     |              whether an exception is raised while calling the Module.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      \n",
      "     |      \n",
      "     |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      "     |      the positional arguments given to the module. Keyword arguments won't be\n",
      "     |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      "     |      input. User can either return a tuple or a single modified value in the\n",
      "     |      hook. We will wrap the value into a tuple if a single value is returned\n",
      "     |      (unless that value is already a tuple). The hook should have the\n",
      "     |      following signature::\n",
      "     |      \n",
      "     |          hook(module, args) -> None or modified input\n",
      "     |      \n",
      "     |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      "     |      kwargs given to the forward function. And if the hook modifies the\n",
      "     |      input, both the args and kwargs should be returned. The hook should have\n",
      "     |      the following signature::\n",
      "     |      \n",
      "     |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``forward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``forward_pre`` hooks registered with\n",
      "     |              :func:`register_module_forward_pre_hook` will fire before all\n",
      "     |              hooks registered by this method.\n",
      "     |              Default: ``False``\n",
      "     |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      "     |              given to the forward function.\n",
      "     |              Default: ``False``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to a module\n",
      "     |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      "     |      respect to module outputs are computed. The hook should have the following\n",
      "     |      signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      "     |              this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients for the module are computed.\n",
      "     |      The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      "     |      \n",
      "     |      The :attr:`grad_output` is a tuple. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      "     |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      "     |      all non-Tensor arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          hook (Callable): The user-defined hook to be registered.\n",
      "     |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      "     |              all existing ``backward_pre`` hooks on this\n",
      "     |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      "     |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      "     |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      "     |              ``backward_pre`` hooks registered with\n",
      "     |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      "     |              all hooks registered by this method.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_load_state_dict_post_hook(self, hook)\n",
      "     |      Registers a post hook to be run after module's ``load_state_dict``\n",
      "     |      is called.\n",
      "     |      \n",
      "     |      It should have the following signature::\n",
      "     |          hook(module, incompatible_keys) -> None\n",
      "     |      \n",
      "     |      The ``module`` argument is the current module that this hook is registered\n",
      "     |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      "     |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      "     |      is a ``list`` of ``str`` containing the missing keys and\n",
      "     |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      "     |      \n",
      "     |      The given incompatible_keys can be modified inplace if needed.\n",
      "     |      \n",
      "     |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      "     |      ``strict=True`` are affected by modifications the hook makes to\n",
      "     |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      "     |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      "     |      clearing out both missing and unexpected keys will avoid an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      "     |      Alias for :func:`add_module`.\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  register_state_dict_pre_hook(self, hook)\n",
      "     |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      "     |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      "     |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      "     |      call is made.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state: Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing references to the whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          The returned object is a shallow copy. It contains references\n",
      "     |          to the module's parameters and buffers.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Currently ``state_dict()`` also accepts positional arguments for\n",
      "     |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      "     |          this is being deprecated and keyword arguments will be enforced in\n",
      "     |          future releases.\n",
      "     |      \n",
      "     |      .. warning::\n",
      "     |          Please avoid the use of argument ``destination`` as it is not\n",
      "     |          designed for end-users.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          destination (dict, optional): If provided, the state of module will\n",
      "     |              be updated into the dict and the same object is returned.\n",
      "     |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      "     |              Default: ``None``.\n",
      "     |          prefix (str, optional): a prefix added to parameter and buffer\n",
      "     |              names to compose the keys in state_dict. Default: ``''``.\n",
      "     |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      "     |              returned in the state dict are detached from autograd. If it's\n",
      "     |              set to ``True``, detaching will not be performed.\n",
      "     |              Default: ``False``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |          recurse (bool): Whether parameters and buffers of submodules should\n",
      "     |              be recursively moved to the specified device.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = True) -> None\n",
      "     |      Resets gradients of all model parameters. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  call_super_init = False\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "\n",
      "FUNCTIONS\n",
      "    get_total_parameters(net)\n",
      "    \n",
      "    n_divide_by_2(x)\n",
      "        # Using binary characteristic for getting max divisable time by 2.\n",
      "\n",
      "FILE\n",
      "    /Users/ysh/Desktop/fun/betago/gomu/base.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gomu.base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898be4d1-bcab-4b9a-8696-6bae634ecd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gomu.base import PolicyValueNet\n",
    "import torch\n",
    "from gomu.gomuku.board import GoMuKuBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7571901f-2897-44c6-a71d-a0d6193ac2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp = torch.load(\"./models/1224-256.pkl\")[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcc4c2d-c74b-448e-8b32-7f59c817a16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyValueNet(\n",
       "  (convs): ModuleList(\n",
       "    (0): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(256, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dropout): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (conv1x): Sequential(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (ff): Sequential(\n",
       "    (0): Rearrange('b c h w -> b (c h w)')\n",
       "    (1): Linear(in_features=400, out_features=1200, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1200, out_features=400, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Linear(in_features=400, out_features=1, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PolicyValueNet(20, 20, [2, 64, 128, 256, 128, 64, 32, 1], dropout=0.0)\n",
    "model.load_state_dict(ckp)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa27295-a6f4-462a-84c8-c240307a1eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = GoMuKuBoard(20, 20, 5)\n",
    "\n",
    "board.set(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cfd05e-bba7-4968-aecd-7f671c394f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_state = torch.from_numpy(board._board).to(dtype=torch.float32)\n",
    "total_BLACK = torch_state[0].sum()\n",
    "total_WHITE = torch_state[1].sum()\n",
    "if total_BLACK != total_WHITE:\n",
    "    torch_state = torch.roll(torch_state, 1, 0)\n",
    "torch_state = torch_state.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "372284b1-af10-468e-b865-522719b71667",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, _ = model(torch_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8ee2416-7b13-48f2-8c5d-a85ba60a1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_free = torch.from_numpy(board._board).sum(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73f2587f-c6f4-4233-8b6e-94c0a6ef9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy * (1-not_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9084fa30-bf47-4d4e-8f0d-a731b5ed179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "policy_ = rearrange(policy, \"B n col row -> B (n col row)\").softmax(1)\n",
    "policyy = policy.view(1, -1).softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3198bd23-07f8-4b3a-8d78-8b03e9bded4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_ == policyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50ff484a-94c4-4f82-88b9-ddd9868422ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normal_dist(x, y, xmu, ymu, sig, temp=10):\n",
    "    return 1/np.sqrt(2*np.pi*pow(sig, 2)) * np.exp((-pow((x-xmu),2)-pow((y-ymu), 2))/(2*pow(sig, 2)))\n",
    "\n",
    "def make_circular_heatmap(ncol, nrow):\n",
    "    xmu = (ncol-1) / 2\n",
    "    ymu = (nrow-1) / 2\n",
    "    sig = (xmu+ymu)/4\n",
    "\n",
    "    heatmap = np.zeros((nrow, ncol))\n",
    "    \n",
    "    for row in range(nrow):\n",
    "        for col in range(ncol):\n",
    "            val = normal_dist(x=col, y=row, xmu=xmu, ymu=ymu, sig=sig)\n",
    "            heatmap[row, col] = val\n",
    "    \n",
    "    return heatmap\n",
    "a = make_circular_heatmap(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "faeb2a22-46ac-42ac-9523-23a36f5c79a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d277972-c94d-41fa-9c20-829362c5f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58ac7fc3-c352-4bbe-96da-822242309a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(range(400)), p=a.reshape(-1)/a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "534dd436-da7c-44c6-95f7-23eaca069ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 18)\n",
      "(6, 8)\n",
      "(9, 7)\n",
      "(4, 3)\n",
      "(2, 7)\n",
      "(10, 13)\n",
      "(8, 9)\n",
      "(13, 10)\n",
      "(12, 4)\n",
      "(5, 11)\n",
      "(1, 7)\n",
      "(11, 12)\n",
      "(6, 16)\n",
      "(15, 14)\n",
      "(13, 16)\n",
      "(8, 12)\n",
      "(13, 13)\n",
      "(12, 8)\n",
      "(16, 19)\n",
      "(12, 15)\n",
      "(11, 11)\n",
      "(11, 11)\n",
      "(12, 6)\n",
      "(4, 6)\n",
      "(2, 12)\n",
      "(9, 15)\n",
      "(17, 5)\n",
      "(10, 4)\n",
      "(12, 0)\n",
      "(10, 4)\n",
      "(1, 6)\n",
      "(4, 12)\n",
      "(9, 10)\n",
      "(9, 5)\n",
      "(8, 1)\n",
      "(19, 12)\n",
      "(4, 14)\n",
      "(10, 6)\n",
      "(8, 13)\n",
      "(16, 6)\n",
      "(9, 17)\n",
      "(15, 12)\n",
      "(10, 11)\n",
      "(10, 2)\n",
      "(1, 0)\n",
      "(14, 8)\n",
      "(9, 11)\n",
      "(13, 9)\n",
      "(15, 3)\n",
      "(9, 4)\n"
     ]
    }
   ],
   "source": [
    "def format_pos(idx, ncol):\n",
    "    return (idx%ncol, idx//ncol)\n",
    "\n",
    "for _ in range(50):\n",
    "    print(format_pos(np.random.choice(list(range(400)), p=a.reshape(-1)/a.sum()), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e30ccf6-b9ed-4005-bdf4-85d9c7cc652c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2daabd43714c77b05c808427afe262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908ca6ff3d7941fd99b1fb794c62eb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from gomu.helpers import DEBUG, GameInfo\n",
    "from gomu.gomuku import GoMuKuBoard\n",
    "from gomu.bot import load_base, PytorchAgent\n",
    "from gomu.algorithms import *\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "nrow, ncol, n_to_win = 20, 20, 5\n",
    "game_info = GameInfo(ncol, nrow, n_to_win)\n",
    "\n",
    "result = 0\n",
    "i = 0\n",
    "\n",
    "base = load_base(game_info=game_info, device=device, ckp_path=\"./models/1224-256.pkl\")\n",
    "model = load_base(game_info=game_info, device=device, ckp_path=\"./models/1224-256.pkl\")\n",
    "base_agent = PytorchAgent(base, device=device, n_to_win=n_to_win, with_history=False)\n",
    "model_agent = PytorchAgent(model, device=device, n_to_win=n_to_win, with_history=False)\n",
    "\n",
    "board = GoMuKuBoard(nrow=nrow, ncol=ncol, n_to_win=n_to_win)\n",
    "turn = 0\n",
    "models_turn = 0\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# _board = GoMuKuBoard.viz(board.board)\n",
    "# display(_board, output)\n",
    "\n",
    "button = widgets.Button(description=\"Next\")\n",
    "display(button, output)\n",
    "\n",
    "button.on_click(on_click_event)\n",
    "\n",
    "is_done = False\n",
    "\n",
    "def on_click_event(b):\n",
    "    global models_turn\n",
    "    global board\n",
    "    global turn\n",
    "    global is_done\n",
    "\n",
    "    if is_done: return\n",
    "          \n",
    "    if models_turn == turn:\n",
    "        agent: PytorchAgent = model_agent\n",
    "    else:\n",
    "        agent: PytorchAgent = base_agent\n",
    "\n",
    "    # Stochastical Variation.\n",
    "    next_pos, value = agent.predict_next_pos(board.board, top_k=3, best=True)\n",
    "    col, row = next_pos[0]\n",
    "    board.set(col, row)\n",
    "\n",
    "    with output:\n",
    "        clear_output()\n",
    "        current_board = GoMuKuBoard.viz(board.board)\n",
    "        # plt.imshow(board_status_img)\n",
    "        print(board)\n",
    "        display(current_board)\n",
    "        \n",
    "    base_agent.update_history(next_pos)\n",
    "    model_agent.update_history(next_pos)\n",
    "    turn = 1 - turn\n",
    "\n",
    "    if board.is_gameover():\n",
    "        if models_turn == turn:\n",
    "            result += 1\n",
    "        if DEBUG >= 2:\n",
    "            GoMuKuBoard.viz(board.board).show()\n",
    "        is_done = True\n",
    "\n",
    "    if board.is_draw():\n",
    "        result += 0.5\n",
    "        is_done = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4e394-78a6-4293-b0f3-037d572c06e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104a4b0-565a-4743-bd6a-9e9b470e2acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bffcb-8a3e-4ea4-8600-771edecb9399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086deaa-6399-4adf-98d3-83e731826a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84085aa2-61bc-46b0-a102-feddadf103ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6541bb-6a60-4328-9336-52c1c909b2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07292a6-102b-4028-804d-821dfc546824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d2639ffb07810fac2cedc92e08a41c0bae42ca785c48ccdb21dd6b5e60bd2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
